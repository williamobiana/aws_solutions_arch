## S3 Overview
What is S3
S3 basics
Working with S3 buckets
Key-Value store
Avaliability and Durability
Storage classes Characteristics
Securing your data
Consistency Model

What is S3? (simple storage service)
* it is an object storage in the cloud (it provide secure, durable and highly scalable object storage)
* it is scaleable (allows to store and retrieve data on the web at a low cost)
* it is simple to use (using web interface)

S3 is object based storage
it manages data as objects (rather than files or blocks)
* you can upload any file type to s3
* examples include photos, video, code, documents and text files
* you CANNOT install or run OS and databases in s3 

S3 basics
* it has unlimited storage 
* it can store object as low as 0Bytes up to 5TB
* we can store our files in buckets (folders)

Working with S3 buckets
* it is a universal namespace (all accounts in the world share the same S3 namespace, each bucket name is unique)
* it has S3 URL (https://``bucket_name``.s3.region.amazonaws.com/``key_name``)
* if upload is successful, you will recieve http status code 200

Key-value store
* Key (name of object)
* Value (data of the object)
* VersionID (used when versioning is enabled)
* Metadata (information about the data, eg: content type, last modified, etc)

Avalaibilty and Durability
Note: S3 is safe, and the data is spread across multiple devices and facilities for avaliablity and durability.
* Built for avaliabilty (99.95-99.99% depending on the s3 tier)
* Designed for durability (99.999999999 (9 decimals total 11 9s) durabilty for the data stored)

Storage Classes
S3 standard is default version of s3
* Highly avalaible and durable (data is stored in >= 3 AZs, 99.99% avalibility, 11 9s durability)
* Designed for frequently accessed data
* Suitable for most workloads (as the default, it can be used for websites, content distribution, mobile/gaming apps, big data, etc)

Tiered Storage

Lifecycle management
* define rules to automatically delete objects or transition objects to a diffrent storage class after a period of time

Versioning 
* all versions of an object are stored and can be retrieved, including deleted objects

Securing data
* Server-side encryption (you can set encryption on a bucket to encrypt and objects stored in it)
* Access control lists ACLs (defines which AWS account or groups are granted access and the type of access, you can attach an ACL to an object in the bucket, thus only those with access can use it)
* Bucket polices (specific to what actions are allows/denied on objects)

Data consistency model
it uses a strong read-after-write consistency 
* after successful write of a new object or overwrite an existing object, any subsequect read request will get the lastest version of the object
* strong consistency for list operations, after a write, we can list all objects in a bucket with all changes reflected

### Exam Tips
S3 is object based storage allowing to upload files
It is not suitable to install an OS or run a database
Files can be from 0 Bytes to 5 Terrabytes
The total volume of data and number of object you can store is unlimited
Files are stored in buckets
S3 is a universal namespace
The bucket has a unique name URL (https://``bucket_name``.s3.region.amazonaws.com/``key_name``)
a succesful CLI or API upload will show a HTTP 200 status code
Object tips to remember:
    Key (name of object)
    Value (data of the object)
    VersionID (used when versioning is enabled)
    Metadata (information about the data, eg: content type, last modified, etc)


## Securing your bucket with S3 Block Public Access
Object ACLs vs Bucket Policies

Object ACLs vs Bucket Policies
* Access control lists ACLs (defines which AWS account or groups are granted access and the type of access, you can attach an ACL to an object in the bucket, thus only those with access can use it)
* Bucket polices (specific to what actions are allows/denied on objects)

ACLs is applied on an object level vs Bucket Policy is applied at the Bucket level

Recap:
S3 is a universal namespace and applied on global regions in console

we can allow objects publicly accesible in the follow demo steps:
1. make the bucket publicly accessible in the permissions tab or during configuration
2. make the object ownership enabled to use ACl in the permission tab or during configuration
3. select the object, in the actions tab select "make public using ACL"
Now we can access the object with the Object URL, and share the object URL with others

### Exam Tips
* Buckets are private by default including all objects inside it, we have to allow public access on both the bucket and object to make the bucket public
* We can make indiviual objects public using object ACL
* We can make entire buckets public using bucket policies
* Successful upload receive HTTP 200 code


## Hosting a Static website using S3
Static website on S3
Automatic Scaling

Static website on S3
You can use S3 to host static websites, such as .html sites
Dynamic websites such as those that need databases connection CANNOT be hosted on S3

S3 scales automatically to meet demand for static websites

### Exam tips
* Make entire bucket public using bucket policies
* We can use S3 to host static content ONLY
* S3 scales automatically with demand


## Versioning Objects in s3
What is versioning?
Advantages of versioning

What is versioning
You can enable versioning in s3 to have multiple versions of the object withing s3

Advantages of versioning
* Allows you to have all versions of the object in s3 (this includes all writes (uploads) even if you delete the object)
* Its a great backup tool
* Once enabled, it cannot be disabled, only suspended
* It intergrates with lifecycle rules
* it supports multifactor authentication
Note: 
each version has it own unique url
Bucket policy does apply to previous version of an object, we have to manual make it public.
if an object is deleted when versioning is turned on, it will not be permanently deleted but saved under a deleted object marker
to restore the object, simply "permernetly delete the object maker" and the object will return

### Exam tips
* All versions of the object are stored in s3 (this includes all writes (uploads) even if you delete the object)
* Its a great backup tool
* Once enabled, it cannot be disabled, only suspended
* It intergrates with lifecycle rules
* it supports multifactor authentication


## Storage classes
s3 standard
s3 standard infrequent access
s3 one zone infrequent access
s3 intelligent tiering
3 glacier options
performance across s3
storage costs

s3 standard
* Highly avalaible and durable (data is stored in >= 3 AZs, 99.99% avalibility, 11 9s durability)
* Designed for frequently accessed data
use case: Suitable for most workloads (as the default, it can be used for websites, content distribution, mobile/gaming apps, big data, etc)

s3 standard infrequent access (s3 standard-IA)
designed for infrequently accesses data
* rapid access for data not used frequently
* pay to access the data (low perGB storage price & low perGB retrieval fee)
use case: great for long term storage, backups, and data store for disaster recovery
99.99% avaliablity, 99.999999999%(11 9s) durability

s3 one zone-IA
similar to s3 standard-IA, but stored in one AZ
* cost 20% less than s3 standard-IA
use case: great for long term , infrequently accesses, non critical data
99.95% avaliablity, 99.999999999%(11 9s) durability

s3 intelligent tiering (2 tiers: frequent and infrequent access)
use case: used when you dont know if you will access your data frequently or infrequently
* it automatically moves your data to the most cost effective tier based on how frequently you access the object
99.99% avaliablity, 99.999999999%(11 9s) durability
optimizes costs at a monthly fee of $0.0025 per 1000 objects

glacier and glacier deep archive
for archiving your data long term
* you pay each time you access your data
* used only for archiving your data
* it is cheap storage
* optimized for data that is infrequently accessed

3 glacier options
* glacier instant retrival: long term data archive with instant retrival time. 
* glacier flexible retrival: ideal storage for archiving data that doesnt need immediate access but flexible to retrieve large data at no cost, this can take minitues up to 12 hours. use case: disaster recovery or backups for non production enviroment data.
* glacier deep archive: cheapest storage for customers that retain thier data sets for 7-10 years or longer to meet customer needs or reglatory compliance. the standard retrival time is 12 hours, bulk retrival time is 48 hours
99.99% avaliablity, 99.999999999%(11 9s) durability across all 3 glacier options

performance accross s3 storage class
* review table comparison online

storage costs
s3 standard = highest cost
s3 intelligent tiering = cost optimized for unknown access patterns
IAs = retrival fee applies

glacier costs
s3 glacier = highest cost + retrival fee
flexible retrival = retrival fee
deep archive = retrival fee

### Exam tips
* recap different types of storage classes and use case
* recap different types of glacier options and use case



## Lifecycle Management with S3
What is lifecycle Management
Lifecycle Management and Versioning

What is lifecycle managemnent?
Automates the moving of objects between the different storage tiers, thereby maximizing cost effectiveness
* s3 standard: keep for 30days
* s3 IA: after 30days
* glacier: after 90days 

We can combining lifecycle management with versioning, to move diffrent versions of object to diffrent storage tiers
lifecycle rules is under the management tab of the bucket

### Exam tips
* Lifecycle rules automates the moving of objects between the different storage tiers
* it can be used in conjuction with versioning
* it can be applied to current or previous versions of an object


## S3 Object Lock and Glacier Vault Lock
s3 object lock
gorvernance mode
compliance mode
retention periods
legal holds
glacier vault lock

s3 object lock
* we can use object lock to store object using ``WORM`` model (write once, read many), this can help prevent objects from being deleted or modified for a fixed time or indefinetly
* you can use s3 lock to meet regulatory requirments that require the WORM model or as extra layer of protection against object changes/deletion

s3 object lock modes
* governance mode: users cannot overwrite/delete an object version or alter its lock settings unless they have permission. (we can grant permission to specific users to alter retention settings or delete the object if necessary)
* compliance mode: a protected object cannot be overwritten/deleted by any user including the root user (this ensures that the object will not be overwritten or deleted during the retention period which cannot be changed or shortened)

retention periods
protects an object version for a fixed amount of time. after retention expires, the object can be overwritten/deleted UNLESS we place a legal hold on the object version.

legal hold
it prevents an object version from being overwritten or deleted, however it doesnt have an associated retention period and will remain on the object until removed. The user with ``s3:PutObjectLegalHold`` permission can place and remove legal holds.

glacier vault lock
easily deploy and enforce controls for individual s3 glacier vaults with a vault lock policy, we can add WORM to the policy and lock the policy from future edits, once locked the policy can no longer be changed.

### Exam tips
* use s3 object lock to store object using ``WORM`` model (Write Once, Read Many)
* object lock can be on individual objects or applied across the bucket as a whole
* object lock comes in 2 modes: governance and compliance
* compliance mode: protected object cannot be overwritten by any use
* governance mode: protected object cannot be overwritten by most users, only user with permission can modify
* s3 glacier vault lock allows to deploy and enforce compliance of the s3 glacier vaults with a vault lock policy containg ``WORM``
* we can specify controls like ``WORM`` in a vault lock policy and lock the policy from future edits, once locked it cant be changed



## Encrypting S3 Objects
Types of encryption
enforcing server side encyrption using bucket policies

Types of encryption
* encryption in transit (ssl/tls, https)
* encryption at rest (server side enryption)
    SSE-S3 (S3 manages keys, AES 256-bit encyption)
    SSE-KMS (AWS key management service managed keys)
    SSE-C (customer provided keys)
* encryption at rest (client side encryption: local encryption of files and upload to s3)

2 ways to enforce encryption
* console: selecting the encryption settings on the s3 bucket with a checkbox
* bucket policy: using a bucket policy

To enforce server side encryption (x-amz-server-side-encryption)
the (x-amz-server-side-encryption) parameter will be included in the put request.
we can create a bucket policy that denys any PUT request that doesnt include the (x-amz-server-side-encryption) parameter

### Exam tips
* encryption in transit (ssl/tls, https)
* encryption at rest (server side enryption)
    SSE-S3 (S3 manages keys, AES 256-bit encyption)
    SSE-KMS (AWS key management service managed keys)
    SSE-C (customer provided keys)
* encryption at rest (client side encryption: local encryption of files and upload to s3)
* To enforce encryption with bucket policy, we can create a bucket policy that denys any PUT request that doesnt include the (x-amz-server-side-encryption) parameter.



## Optimizing S3 Performance
s3 prefixes
s3 performance
limitations with kms
s3 performance uploads
s3 performance downloads

s3 prefixes
bucketname/folder1/subfolder1/object.jpg (the prefix is the /folder1/subfolder1 in the bucketname, it doesnt include the object name)

s3 performance
* s3 has extremly low latency, we can get the first byte within 100-200 milliseconds
* 3500 PUT/COPY/POST/DELETE, 5500 GET/HEAD requests per second per prefix
* The more folders and subfolders we have the better the performance

s3 limitations
* if we are using SSE-KMS to encrypt objects in s3, remeber KMS has built in limits
* when you upload a file, you will call GenerateDataKey in KMS api
* when you download a file, you call Decrypt in KMS api

KMS request rates
* uploading/downloading will count tomwards the KMS quota
* we cannot currently request a quota increase for KMS
* region-specific, however it is either 5500, 10000, or 30000 requests per second

s3 performance uploads (multi-part uploads)
* recommended for files over 100MB
* required for files over 5GB
* parallelize uploads (increase efficency by breaking a big file into parts and uploading simutaneously)

s3 performance downloads (s3 byte range fetches)
* parallelize downloads by specifying byte range
* if there is failure in the download, it is only for a specific byte range
* it can be used to speed up downloads
* can be used to download partial amounts of the file (eg: header information)

### Exam tips
* s3 prefix is the /folder1/subfolder1 in the s3 bucketname, it doesnt include the object name
* the more prefix you use with the s3, the better performance you will get
* you can achieve a high number of requests 3500 PUT/COPY/POST/DELETE, 5500 GET/HEAD requests per second per prefix
* you can get better performance by spreading your reads across diffrent prefixes.
* if you are using SSE-KMS to encrypt objects, we should remember it has a KMS limit
* uploading/downloading will count tomwards the KMS quota
* we cannot currently request a quota increase for KMS
* region-specific, however it is either 5500, 10000, or 30000 requests per second
* use multi-parts uploads to increase performance when uploading files to s3
* recommended for files over 100MB
* use s3 byte range fetches to increase perfomance when downloading files to s3


## Backing up Data with s3 Replication
S3 replication

you can replicate objects from 1 bucket to another,
Versioning must be enabled in source and destination buckets

Objects in an existing bucket are not replicated automatically
once replication is turned on, then subsequent object can be repliated automatically

Delete markers are not replicated by default

Replication rules are in the management tab of the bucket

### Exam tips
* you can replicate objects from one bucket to another
* objects in an existing bucket are not replicated automatically
* delete markers are not replicated by default


## Exam Tips Recap
### Exam Tips1 (overview)
S3 is object based storage allowing to upload files
It is not suitable to install an OS or run a database
Files can be from 0 Bytes to 5 Terrabytes
The total volume of data and number of object you can store is unlimited
Files are stored in buckets
S3 is a universal namespace
The bucket has a unique name URL (https://``bucket_name``.s3.region.amazonaws.com/``key_name``)
a succesful CLI or API upload will show a HTTP 200 status code
Object tips to remember:
    Key (name of object)
    Value (data of the object)
    VersionID (used when versioning is enabled)
    Metadata (information about the data, eg: content type, last modified, etc)

### Exam Tips2 (secure the bucket)
* Buckets are private by default including all objects inside it, we have to allow public access on both the bucket and object to make the bucket public
* We can make indiviual objects public using object ACL
* We can make entire buckets public using bucket policies
* Successful upload receive HTTP 200 code

### Exam tips3 (hosting a static website)
* Make entire bucket public using bucket policies
* We can use S3 to host static content ONLY
* S3 scales automatically with demand

### Exam tips4 (versioning)
* All versions of the object are stored in s3 (this includes all writes (uploads) even if you delete the object)
* Its a great backup tool
* Once enabled, it cannot be disabled, only suspended
* It intergrates with lifecycle rules
* it supports multifactor authentication

### Exam tips5 (storage class)
* recap different types of storage classes and use case
* recap different types of glacier options and use case

### Exam tips6 (lifecycle management)
* Lifecycle rules automates the moving of objects between the different storage tiers
* it can be used in conjuction with versioning
* it can be applied to current or previous versions of an object

### Exam tips7 (object lock and glacier vault)
* use s3 object lock to store object using ``WORM`` model (Write Once, Read Many)
* object lock can be on individual objects or applied across the bucket as a whole
* object lock comes in 2 modes: governance and compliance
* compliance mode: protected object cannot be overwritten by any use
* governance mode: protected object cannot be overwritten by most users, only user with permission can modify
* s3 glacier vault lock allows to deploy and enforce compliance of the s3 glacier vaults with a vault lock policy containg ``WORM``
* we can specify controls like ``WORM`` in a vault lock policy and lock the policy from future edits, once locked it cant be changed

### Exam tips8 (encryption)
* encryption in transit (ssl/tls, https)
* encryption at rest (server side enryption)
    SSE-S3 (S3 manages keys, AES 256-bit encyption)
    SSE-KMS (AWS key management service managed keys)
    SSE-C (customer provided keys)
* encryption at rest (client side encryption: local encryption of files and upload to s3)
* To enforce encryption with bucket policy, we can create a bucket policy that denys any PUT request that doesnt include the (x-amz-server-side-encryption) parameter.

### Exam tips9 (optimizing)
* s3 prefix is the /folder1/subfolder1 in the s3 bucketname, it doesnt include the object name
* the more prefix you use with the s3, the better performance you will get
* you can achieve a high number of requests 3500 PUT/COPY/POST/DELETE, 5500 GET/HEAD requests per second per prefix
* you can get better performance by spreading your reads across diffrent prefixes.
* if you are using SSE-KMS to encrypt objects, we should remember it has a KMS limit
* uploading/downloading will count tomwards the KMS quota
* we cannot currently request a quota increase for KMS
* region-specific, however it is either 5500, 10000, or 30000 requests per second
* use multi-parts uploads to increase performance when uploading files to s3
* recommended for files over 100MB
* use s3 byte range fetches to increase perfomance when downloading files to s3

### Exam tips10 (backing up data)
* you can replicate objects from one bucket to another
* objects in an existing bucket are not replicated automatically
* delete markers are not replicated by default

